{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经机器翻译系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用TensorFlow搭建seq2seq模型实现了一个简单的神经机器翻译系统，实现英语翻译为法语。Encoder使用双向LSTM。Decoder采用了attention机制。\n",
    "\n",
    "使用多张计算图分别处理train，eval和infer，并分别在不同的session中进行训练和推断。参数共享用Saver。\n",
    "\n",
    "增加了tensorboard可视化。decoder的initial state采用双向encoder state的平均值。\n",
    "\n",
    "详见TensorFlow教程：https://tensorflow.google.cn/tutorials/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包\n",
    "检查TensorFlow版本和GPU情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'en-fr/small_vocab_en'\n",
    "target_path = 'en-fr/small_vocab_fr'\n",
    "checkpoint_path = './tmp-model.ckpt'\n",
    "batch_size = 256\n",
    "num_units = 32\n",
    "num_layers = 2\n",
    "max_gradient_norm = 5.0\n",
    "learning_rate = 0.001\n",
    "epoch = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立lookup table文件\n",
    "sos句子开始。eos句子结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "with open(source_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l += line.split()\n",
    "f.close()\n",
    "unique_words_src = ['eos'] + list(set(l))\n",
    "\n",
    "with open('en-fr/words_en', 'w', encoding='utf-8') as f:\n",
    "    for word in unique_words_src:\n",
    "        f.write(word + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961295 357\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "with open(target_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l += line.split()\n",
    "f.close()\n",
    "unique_words_tar = ['sos'] + ['eos'] + list(set(l))\n",
    "print(len(l), len(unique_words_tar))\n",
    "#print ('rusty' in unique_words_en)\n",
    "\n",
    "with open('en-fr/words_fr', 'w', encoding='utf-8') as f:\n",
    "    for word in unique_words_tar:\n",
    "        f.write(word + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用预训练词向量 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file_src = os.path.join('.', 'fasttext', 'wiki-news-300d-1M.vec')\n",
    "embed_file_tar = os.path.join('.', 'fasttext', 'cc.fr.300.vec')\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_src = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embed_file_src, encoding = 'utf-8'))\n",
    "embeddings_index_tar = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embed_file_tar, encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立两种语言的embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_src = np.random.normal(size=(len(unique_words_src), embed_size), scale=0.01)\n",
    "embedding_matrix_tar = np.random.normal(size=(len(unique_words_tar), embed_size), scale=0.01)\n",
    "\n",
    "for i, word in enumerate(unique_words_src):\n",
    "    embedding_vector = embeddings_index_src.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_src[i] = embedding_vector\n",
    "        \n",
    "for i, word in enumerate(unique_words_tar):\n",
    "    embedding_vector = embeddings_index_tar.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_tar[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成lookup table函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BuildLookupTable(source_words_path, target_words_path):\n",
    "    lookup_src = tf.contrib.lookup.index_table_from_file(source_words_path)\n",
    "    lookup_tar = tf.contrib.lookup.index_table_from_file(target_words_path)\n",
    "    lookup_translate = tf.contrib.lookup.index_to_string_table_from_file(target_words_path)\n",
    "    return lookup_src, lookup_tar, lookup_translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入训练文本预处理函数\n",
    "预处理source和target dataset。文本转成单词id。target开头加一个sos。分batch并pad。末尾用eos补足到最大长度。\n",
    "这里不需要drop remainder，iterator会自动计算最后一批的样本量。但是后面不能再使用batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTrainDataset(source_path, target_path, lookup_src, lookup_tar, src_eos_id, tar_eos_id):\n",
    "    \n",
    "    # source\n",
    "    source_dataset = tf.data.TextLineDataset(source_path)\n",
    "    source_dataset = source_dataset.map(lambda string: tf.string_split([string]).values)\n",
    "    source_dataset = source_dataset.map(lambda words: (words, tf.size(words)))\n",
    "    source_dataset = source_dataset.map(lambda words, size: (lookup_src.lookup(words), size))\n",
    "    \n",
    "    # target\n",
    "    target_dataset = tf.data.TextLineDataset(target_path)\n",
    "    target_dataset = target_dataset.map(lambda string: tf.string_split([tf.string_join([tf.constant('sos'), string], separator=' ')]).values)\n",
    "    target_dataset = target_dataset.map(lambda words: (words, tf.size(words)))\n",
    "    target_dataset = target_dataset.map(lambda words, size: (lookup_tar.lookup(words), size))\n",
    "    \n",
    "    # zip source and target\n",
    "    source_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))\n",
    "\n",
    "    # batch and pad\n",
    "    batched_dataset = source_target_dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size\n",
    "                        tf.TensorShape([])),     # size(source)\n",
    "                       (tf.TensorShape([None]),  # target vectors of unknown size\n",
    "                        tf.TensorShape([]))),    # size(target)\n",
    "        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id\n",
    "                         0),          # size(source) -- unused\n",
    "                        (tar_eos_id,  # target vectors padded on the right with tar_eos_id\n",
    "                         0)))         # size(target) -- unused\n",
    "    \n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the train model function\n",
    "Input: batched and padded dataset iterator ((source, source_lengths), (target, target_lengths))\n",
    "\n",
    "Output: command to run in train session\n",
    "\n",
    "这里注意state的形状，传给decoder要对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTrainModel(train_iterator):\n",
    "    ((source, source_lengths), (target, target_lengths)) = train_iterator.get_next()\n",
    "    encoder_inputs = tf.transpose(source, [1,0]) # to time major\n",
    "    decoder_inputs = tf.transpose(target, [1,0])\n",
    "    decoder_outputs = tf.pad(decoder_inputs[1:], tf.constant([[0,1],[0,0]]), constant_values=tar_eos_id)\n",
    "\n",
    "    shape = tf.shape(decoder_outputs)\n",
    "    target_weights = tf.to_double(tf.where(tf.equal(decoder_outputs, tf.fill(shape, tar_eos_id)), tf.zeros(shape), tf.ones(shape)))\n",
    "            \n",
    "    embedding_encoder = tf.Variable(embedding_matrix_src, name='embedding_encoder')\n",
    "    embedding_decoder = tf.Variable(embedding_matrix_tar, name='embedding_decoder')\n",
    "    \n",
    "    # Embedding layer\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    # Construct forward and backward cells\n",
    "    forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    encoder_outputs, encoder_states_fw, encoder_states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        [forward_cell] * num_layers, [backward_cell] * num_layers, encoder_emb_inp, dtype=tf.float64, \n",
    "        sequence_length=source_lengths, time_major=True)\n",
    "    #encoder_states: the final states, one tensor per layer, of the forward/backward rnn\n",
    "\n",
    "    # Attention\n",
    "    attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        num_units, attention_states, memory_sequence_length=source_lengths, dtype=tf.float64)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, name = 'decoder_cell')\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([decoder_cell] * num_layers)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "    initial_state = decoder_cell.zero_state(dtype=tf.float64, batch_size=tf.shape(encoder_inputs)[1])\n",
    "    initial_state = initial_state.clone(\n",
    "        cell_state = encoder_states_fw)\n",
    "    \n",
    "    # Projection layer on the top\n",
    "    projection_layer = tf.layers.Dense(len(unique_words_tar), use_bias=False, name='projection')\n",
    "    \n",
    "    # Decoder for training\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, target_lengths, time_major=True)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state, output_layer=projection_layer)\n",
    "    outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, impute_finished=True)\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    # Loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)\n",
    "    train_loss = tf.reduce_sum(crossent * target_weights)/ tf.to_double(tf.shape(encoder_inputs)[1])\n",
    "    tf.summary.scalar('train_loss', train_loss)\n",
    "    \n",
    "    # Gradient\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    return train_loss, update_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置train graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 96 and 332 for 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,96], [332,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1566\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1567\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1568\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 96 and 332 for 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,96], [332,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f2f947712740>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Build the train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtrain_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatched_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBuildTrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtable_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-eaf6570317de>\u001b[0m in \u001b[0;36mBuildTrainModel\u001b[1;34m(train_iterator)\u001b[0m\n\u001b[0;32m     22\u001b[0m     encoder_outputs, encoder_states_fw, encoder_states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n\u001b[0;32m     23\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mforward_cell\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbackward_cell\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_emb_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         sequence_length=source_lengths, time_major=True)\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#encoder_states: the final states, one tensor per layer, of the forward/backward rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mstack_bidirectional_dynamic_rnn\u001b[1;34m(cells_fw, cells_bw, inputs, initial_states_fw, initial_states_bw, dtype, sequence_length, parallel_iterations, time_major, scope)\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             time_major=time_major)\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;31m# Concat the outputs to create the new input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mprev_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[1;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    410\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;31m# Backward direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[0;32m   3222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3224\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3226\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2955\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2956\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2957\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2958\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2891\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m   2892\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3192\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3193\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3194\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    791\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m           skip_conditionals=True)\n\u001b[0m\u001b[0;32m    794\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_rnn_step\u001b[1;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mnew_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[1;32m--> 339\u001b[1;33m                                      *args, **kwargs)\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, state)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     gate_inputs = math_ops.matmul(\n\u001b[1;32m--> 620\u001b[1;33m         array_ops.concat([inputs, h], 1), self._kernel)\n\u001b[0m\u001b[0;32m    621\u001b[0m     \u001b[0mgate_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgate_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2122\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   4276\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   4277\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4278\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   4279\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4280\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3392\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m       \u001b[1;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1734\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1735\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1568\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 96 and 332 for 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,96], [332,128]."
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Build the lookup table\n",
    "    lookup_src, lookup_tar, lookup_translate = BuildLookupTable('en-fr/words_en', 'en-fr/words_fr')\n",
    "    \n",
    "    # set the sos and eos\n",
    "    src_eos_id=lookup_src.lookup(tf.constant('eos')) #0 in source vocab\n",
    "    tar_sos_id=lookup_tar.lookup(tf.constant('sos')) #0 in target vocab\n",
    "    tar_eos_id=lookup_tar.lookup(tf.constant('eos')) #1 in target vocab\n",
    "    \n",
    "    # Preprocess the text dataset\n",
    "    batched_dataset = BuildTrainDataset(source_path, target_path, lookup_src, lookup_tar, src_eos_id, tar_eos_id)\n",
    "    \n",
    "    # Build the train model\n",
    "    train_iterator = batched_dataset.make_initializable_iterator()\n",
    "    train_model = BuildTrainModel(train_iterator)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    table_initializer = tf.tables_initializer()\n",
    "    train_saver = tf.train.Saver(max_to_keep=2)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./vis/train', train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the train session to train the model and save the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sess = tf.Session(graph=train_graph)\n",
    "train_sess.run(initializer)\n",
    "train_sess.run(table_initializer)\n",
    "train_sess.run(train_iterator.initializer)\n",
    "\n",
    "#saver.restore(sess, './tmp-model.ckpt-11')\n",
    "for i in tqdm(range(epoch)):\n",
    "    train_sess.run(train_iterator.initializer)\n",
    "    n_batch=0\n",
    "    while True:\n",
    "        try:\n",
    "            summary, (cost, _) = train_sess.run([merged, train_model])\n",
    "            train_writer.add_summary(summary)\n",
    "            n_batch+=1\n",
    "            print (n_batch)\n",
    "            if n_batch % 10 == 0:\n",
    "                print (cost)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print (cost)\n",
    "            break\n",
    "    model_path = train_saver.save(train_sess, checkpoint_path, global_step=i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入测试文本预处理函数\n",
    "预处理用于infer的source dataset。文本转成单词id。分batch并pad。末尾用eos补足到最大长度。\n",
    "这里不需要drop remainder，iterator会自动计算最后一批的样本量。但是后面不能再使用batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTestDataset(source_path, lookup_src, src_eos_id):\n",
    "    \n",
    "    # source\n",
    "    source_dataset = tf.data.TextLineDataset(source_path)\n",
    "    source_dataset = source_dataset.map(lambda string: tf.string_split([string]).values)\n",
    "    source_dataset = source_dataset.map(lambda words: (words, tf.size(words)))\n",
    "    source_dataset = source_dataset.map(lambda words, size: (lookup_src.lookup(words), size))\n",
    "\n",
    "    # batch and pad\n",
    "    batched_dataset = source_dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(tf.TensorShape([None]),  # source vectors of unknown size\n",
    "                        tf.TensorShape([])),     # size(source)\n",
    "        padding_values=(src_eos_id,  # source vectors padded on the right with src_eos_id\n",
    "                         0))          # size(source) -- unused\n",
    "    \n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the infer model function\n",
    "Input: batched and padded dataset iterator ((source, source_lengths), (target, target_lengths))\n",
    "\n",
    "Output: command to run in infer session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildInferModel(test_iterator, tar_sos_id, tar_eos_id):\n",
    "    (source, source_lengths) = test_iterator.get_next()\n",
    "    encoder_inputs = tf.transpose(source, [1,0])\n",
    "    \n",
    "    embedding_encoder = tf.Variable(embedding_matrix_src, name='embedding_encoder')\n",
    "    embedding_decoder = tf.Variable(embedding_matrix_tar, name='embedding_decoder')\n",
    "    \n",
    "    # Embedding layer\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    # Construct forward and backward cells\n",
    "    forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, name = 'forward_cell')\n",
    "    backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, name = 'backward_cell')\n",
    "\n",
    "    bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        forward_cell, backward_cell, encoder_emb_inp, dtype=tf.float64,\n",
    "        sequence_length=source_lengths, time_major=True)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    #encoder_state: tuple of 2 LSTM state tuple ((cell_state_fw, hidden_state_fw), (cell_state_bw, hidden_state_bw))\n",
    "    \n",
    "    # Attention\n",
    "    attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        num_units, attention_states, memory_sequence_length=source_lengths, dtype=tf.float64)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, name = 'decoder_cell')\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "    initial_state = decoder_cell.zero_state(dtype=tf.float64, batch_size=tf.shape(encoder_inputs)[1])\n",
    "    initial_state = initial_state.clone(\n",
    "        cell_state = tf.contrib.rnn.LSTMStateTuple((encoder_state[0].c+encoder_state[1].c)/2.0, (encoder_state[0].h+encoder_state[1].h)/2.0))\n",
    "    \n",
    "    # Projection layer on the top\n",
    "    projection_layer = tf.layers.Dense(len(unique_words_tar), use_bias=False, name='projection')\n",
    "    \n",
    "    # Decoder to infer\n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding_decoder, tf.fill([tf.shape(encoder_inputs)[1]], tf.to_int32(tar_sos_id)), tf.to_int32(tar_eos_id))\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, infer_helper, initial_state, output_layer=projection_layer)\n",
    "    maximum_iterations = tf.round(tf.reduce_max(source_lengths) * 2)\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=maximum_iterations, output_time_major=True, impute_finished=True)\n",
    "    \n",
    "    translation_id = tf.to_int64(outputs.sample_id)\n",
    "    translation = lookup_translate.lookup(translation_id)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置infer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_graph = tf.Graph()\n",
    "with infer_graph.as_default():\n",
    "    \n",
    "    # Build the lookup table\n",
    "    lookup_src, lookup_tar, lookup_translate = BuildLookupTable('en-fr/words_en', 'en-fr/words_fr')\n",
    "    \n",
    "    # set the sos and eos\n",
    "    src_eos_id=lookup_src.lookup(tf.constant('eos')) #0 in source vocab\n",
    "    tar_sos_id=lookup_tar.lookup(tf.constant('sos')) #0 in target vocab\n",
    "    tar_eos_id=lookup_tar.lookup(tf.constant('eos')) #1 in target vocab\n",
    "    \n",
    "    # Preprocess the text dataset\n",
    "    batched_dataset = BuildTestDataset(source_path, lookup_src, src_eos_id)\n",
    "    \n",
    "    # Build the train model\n",
    "    test_iterator = batched_dataset.make_initializable_iterator()\n",
    "    infer_model = BuildInferModel(test_iterator, tar_sos_id, tar_eos_id)\n",
    "    infer_saver = tf.train.Saver()\n",
    "    table_initializer = tf.tables_initializer()\n",
    "    infer_writer = tf.summary.FileWriter('./vis/infer', infer_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the infer session to translate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_sess = tf.Session(graph=infer_graph)\n",
    "infer_sess.run(table_initializer)\n",
    "infer_saver.restore(infer_sess, model_path)\n",
    "infer_sess.run(test_iterator.initializer)\n",
    "n_batch=0\n",
    "f = open('en-fr/trans_fr', 'w', encoding='utf-8')\n",
    "while True:\n",
    "    try:\n",
    "        tar_sentences = infer_sess.run(infer_model)\n",
    "        n_batch+=1\n",
    "        tar_sentences = np.transpose(tar_sentences)\n",
    "        for sentence in tar_sentences:\n",
    "            for word in sentence:\n",
    "                if word == 'eos': break\n",
    "                f.write(word.decode('utf-8') + ' ')\n",
    "            f.write('\\n')\n",
    "        print(n_batch)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "f.close()\n",
    "\n",
    "# Close the session manually and release resources\n",
    "train_sess.close()\n",
    "infer_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess: #debug\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.tables_initializer().run()\n",
    "    sess.run(batched_iterator.initializer)\n",
    "    n_batch=0\n",
    "    ei, di, do = sess.run([encoder_inputs, decoder_inputs, decoder_outputs])\n",
    "            #print (np.shape(ei), np.shape(di), np.shape(do))\n",
    "    n_batch+=1\n",
    "    print(n_batch)\n",
    "    model_path = saver.save(sess, './tmp-model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
